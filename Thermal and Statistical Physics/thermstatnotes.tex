\documentclass[12pt, oneside]{book}
\usepackage{amsmath, amssymb, amssymb, mathtools, graphicx, hyperref, titling}
\usepackage{tikz, caption, subcaption, enumitem, polyglossia}
\usepackage{tikz-3dplot, float, etoolbox, enumitem}
\usepackage[top=25mm, bottom=25mm, left=20mm, right=20mm]{geometry}
%\usepackage{showframe}

\usepackage{mathtools}
\DeclarePairedDelimiter{\evdel}{\langle}{\rangle}
\newcommand{\ev}{\evdel}

\makeatletter
\patchcmd{\makechapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter head
\patchcmd{\makeschapterhead}{\vspace*{50\p@}}{}{}{}% Removes space above \chapter* head
\makeatother

\predate{}
\postdate{}
\date{}
\title{PH2223 - Thermal and\\Statistical Physics}
\author{Nachiketa Kulkarni}
\pagenumbering{gobble}
\setmainfont{Comic Neue}

\begin{document}
\maketitle
\tableofcontents

\mainmatter
\chapter{Preliminaries}
\section{Introduction}
\subsection{Large Numbers - What is a mole?}
Population of India: \( \approx 1.4 \times 10^9\)\\
Indian Economy: \( \approx 4 \times 10^{12} \) USD\\
Number of \(\text{N}_2\) molecules in 1kg of Nitrogen: \( \approx 10^{25} \)\\

This number is too big.
Instead of dealing with numbers this large.
We will be dealing with averages for these large set of numbers.

Consider the addition of two numbers \(a_1\) and \(a_2\), given they are of the same order of magnitude.
As we increase the number of terms, the sum increases, but overall sum is not going to change by a significant value.
As we approach \(100\) terms, another term added will only result in minor fluctuations in the sum. 

We use the term mole as a unit to represent the number of particles in a system.
\paragraph{Mole:}A mole is defined as the quantity of matter that contains as many entities as the number of atoms in exactly \(12\text{g}\) of \(^{12}\text{C}\).
This number is represented as \(N_A\) and is approximated to:
\[ N_A = 6.022 \times 10^{23} \]

\paragraph{Thermodynamic Limit:} When the number of elements in our system increases to a number large enough such that the fluctuations in the sum is very little is known as the Thermodynamic Limit.
This is the point where we can deal with averages and their distributions instead of precise values.

\subsection{Combinatorics}
In the above examples, we are merely counting the number of objects present.
But often times we will be dealing with the various ways in which the parameters of these objects can be changed and how that can affect the overall macroscopic system.

\paragraph{Logarithm and Sterling's Approximation} When dealing with very large numbers we will be using logarithms(often base \(e\)) to make the large numbers that we have to deal with down to a size that is easier to understand.
As we will be dealing with large numbers and their factorials, we will be using the Sterling's approximation to simplify the logarithms:
\[ \ln(n!) \approx n \ln(n) - n \]

\subsection{Properties of Gas}
There are many properties that can be used to describe the state of a Gas.
Eg: Volume \(V\), Internal Energy \(U\), Pressure \(P\), Temperature \(T\).

These properties are broadly classified as:
\begin{enumerate}
    \item \textbf{Intensive Properties} Properties that are not affected by the size of the system. Eg: \(P, T\)
    \item \textbf{Extensive Properties} Properties that scale with the size of system. Eg: \(V, U\)
\end{enumerate}

\subsection{Ideal Gas}
Experiments on gases show some of the relations between \(P\), \(V\) and \(T\)
\begin{enumerate}
    \item \textbf{Boyle's Law} A fixed amount of gas at a constant temperature obeys:
    \[P \propto \frac{1}{V}\]
    \item \textbf{Charles' Law} A fixed amount of gas at a constant pressure obeys:
    \[ V \propto T \]
    where T is measure in Kelvin.
    \item \textbf{Gay-Lussac's Law} A fixed amount of gas at a constant volume obeys:
    \[ P \propto T \]
\end{enumerate}
These three laws can be combined to give the following relation:
\[ PV \propto T \]
Now, if we consider that there are \(N\) molecules in the gas:
\[ PV = Nk_bT \]
where \(k_b\) is the Boltzmann constant.

\section{Heat}
\subsection{Definition}
Heat is defined as the thermal energy in transit.
Heat is never stored in objects.
It is only energy that is stored.
Heat naturally only flows from a body with higher temperature to a body with lower temperature.
Though, when there is an external energy is provided, heat can flow from a colder body to a hotter body.
\subsection{Heat Capacity}
Heat is not stored.
But Physics has conventionally called this Heat Capacity, so we will call it Heat Capacity.
\paragraph{Heat Capacity} It is the amount of heat(energy) \(dQ\) that is required to raise the temperature of the system by a small amount \(dT\).
\[\text{Heat Capacity }C = \frac{dQ}{dT} \]
Heat Capacity of the system \(C\) per unit mass is known as the Specific Heat Capacity \(c\).

Often times the heat provided to the system isn't utilized fully just to increase temperature.
There are often other constraints to the system, like if the heat is used to do work on the surroundings.
In these cases, we will be using terms like Heat Capacity at constant pressure \(C_P\) or Heat Capacity at constant Volume \(C_V\)

\section{Probability}
Probability is required in Statistical Mechanics because a lot of the phenomenon that occur predicted by the physics are often with a given with a statistical background.
Eg: Probability of various macrostates of a system.

\subsection{Discrete Probability Distributions}
Discrete Random Variables take a finite (or countably infinite) values.
Examples include the number of heads in a coin toss experiment, number of sibblings per person, etc.
Let x be a discrete random variable, and \(x_i\) be the possible values of \(x\).
The probability of the random variable taking the value \(x_i\) is denoted by \(p_i\).
Some of the properties include:
\begin{enumerate}
    \item Sum of probabilities of all random variables add up to \(1\):
    \[\sum_{i} p_i = 1\]
    \item Mean or expectation value of the random variable is defined as:
    \[ \ev{x} = \sum_{i} x_i p_i \]
    \item Any function of x can be averaged as:
    \[ \ev{f(x)} = \sum_{i} f(x_i) p_i \]
\end{enumerate}

\subsection{Continuous Probability Distributions}
Continuous Random Variables take values in a continuous range, i.e., they take uncountably infinite values.
Examples include height of a person, time a train gets delayed by, etc.
We define a Probability Distribution Function \(P(x)\) that says the random variable x has a probability of \(P(x)dx\) of being in the range \([x, x+dx]\).
Some of the properties include:
\begin{enumerate}
    \item The probability of the random variable being in the entire range is \(1\):
    \[ \int_{-\infty}^{\infty} P(x)dx = 1 \]
    \item The expectation value of the random variable is defined as:
    \[ \ev{x} = \int_{-\infty}^{\infty} xP(x)dx \]
    \item Any function of x can be averaged as:
    \[ \ev{f(x)} = \int_{-\infty}^{\infty} f(x)P(x)dx \]
\end{enumerate}

\subsection{Linear Transformations and Translation}
Some times, we have a random variable \(x\), but need to make a second random variable by performing Linear Transformations and/or translations of the initial variable:
\[ \ev{y} = \ev{mx+c} = m\ev{x} + c \]

\subsection{Variance}
Consider a random variable \(x\).
When we are calculating the mean (or expectation value) of \(x\), we often times lose other aspects of the information that we used to get the said value of \(x\).
We use Variance, and standard deviation, to measure the accuracy and the spread of the values of \(x\).
\[ \text{Deviation} = x - \ev{x} \]
To ensure that the negative and positive values are equally take into consideration, and also do not cancel out, we take modulus:
\[ \text{Deviation} = \left|x - \ev{x}\right| \]
Finally, as it is easier to work with squares:
\[ \text{Deviation}^2 = \left(x - \ev{x}\right)^2 \]
The average of this value is known as the variance of \(x\), \(\sigma_x^2\).
\begin{align*}
    \sigma_x^2 &= \ev{\left(x - \ev{x}\right)^2} \\
    \sigma_x^2 &= \ev{x^2} - \ev{x}^2
\end{align*}
The positive square root of variance is called standard deviation about the mean, \(\sigma_x\)
\[ \sigma_x = \sqrt{\ev{x^2} - \ev{x}} \]

\paragraph{Affect of Linear Transformations on Variance}If we consider a random variable \(y = mx + c\), then the variance of \(y\) is:
\begin{align*}
    \sigma_y^2 &= \ev{\left(y - \ev{y}\right)^2} \\
    \sigma_y^2 &= \ev{\left(mx + c - m\ev{x} - c\right)^2} \\
    \sigma_y^2 &= m^2\ev{\left(x - \ev{x}\right)^2} \\
    \sigma_y^2 &= m^2\sigma_x^2
\end{align*}
Considering this, the standard deviation of \(y\) is:
\[ \sigma_y = \left|m\right|\sigma_x \]

\subsection{Independence of Random Variables}
Two random variables are said to be independent if the probability of one random variable taking a value is not affected by the value of the other random variable.

\noindent If \(x\) and \(y\) are independent random variables, then the probability of x being in the range \(\left(x, x+dx\right)\) and y being in the range \(\left(y, y+dy\right)\) is \(P_x \left(x\right) dx P_y \left(y\right) dy \).
The average value of the product of the two random variables is:
\begin{align*}
    \ev{xy} &= \int \int xy P_x \left(x\right) P_y \left(y\right) dx dy \\
    \ev{xy} &= \int x P_x \left(x\right) dx \int y P_y \left(y\right) dy \\
    \ev{xy} &= \ev{x} \ev{y}
\end{align*}

\subsubsection{Experiments and Random Variables}
Normally, when performing experiments, we perform multiple trials and record the results.
In these cases, we can consider the individual trials as random variables, and thus the average of the trials as the expectation value of the random variable.
Also, if the error in each trial is \(\sigma_X\), the error in the rms error in the average of the trials is: \(\sigma_X/\sqrt{N}\)

\subsubsection{Theory of Random Walks}
Consider a drunk person walking along a straight line on a street.
Assume that the length of each step is \(1\) unit.
The drunk person can take a step in either direction with equal probability, and each step is an independent event.
Hence, the expectation value of the position of the drunk person after \(N\) steps is \(0\).
However, the rms distance of the drunk person from the origin is \(n \ev{X}\).
Hence, the rms distance of the drunk person from the origin is \(\sqrt{N}\) units.

\subsection{Binomial Distribution}
\paragraph{Bernoulli Trials} A Bernoulli trial is an experiment with two possible outcomes: Success or Failure.
The probability of success is \(p\), and the probability of failure is \(1-p\).

\paragraph{Binomial Distribution} A Binomial Distribution is the discrete probability distribution \(P\left(n,k\right)\) of getting \(k\) successes in \(n\) Bernoulli trials.
By basic combinatorics, the probability of getting \(k\) successes in \(n\) trials is:
\[ P\left(n,k\right) = \binom{n}{k} p^k \left(1-p\right)^{n-k} \]

\paragraph{Note:} Taking the summation of the Binomial Distribution over all possible values of \(k\):
\[ \sum_{k=0}^{n} P\left(n,k\right) = 1 \]
Hence, it is a valid probability distribution.

\subsubsection{Expectation and Variance of Binomial Distribution}
The expectation value of the Binomial Distribution is:
\[ \ev{k} = np \]
The variance of the Binomial Distribution is:
\[ \sigma_k^2 = np\left(1-p\right) \]
Finally, another important property of the Binomial Distribution is the fractional width of the distribution, \(\sigma_k/\ev{k}\).
\[ \frac{\sigma_k}{\ev{k}} = \sqrt{ \frac{1-p}{np} } \]
As the number of trials increases, the fractional width of the distribution decreases making it narrower about the mean.

\section{Temperature and Boltzmann Constant}
\subsection{Thermal Equilibrium}
Consider two bodies, one which is "hot" and another which is "cold".
As heat is "thermal energy in transit", and given that the system is left alone, heat will flow from the "hot" body to the "cold" body.
In this process, the two bodies would change in temperature and also there would be a change in their energies over time.
Using this logic, Thermal Equilibrium is defined.
\paragraph{Thermal Equilibrium} If there is no net heat flow from either body to the other, the two bodies are said to be in Thermal Equilbrium.
In this state, any heat flowing from the first body to the second body, is the same amount of heat flowing from the second body to the first.
This process is irreversible, i.e., the heat transfer from the "hot" body to the "cold" body is one directional, and is not reversible.
\paragraph{Zeroth Law of Thermodynamics} Two systems, each separately in the thermal equilibrium with a third, are in equilibrium with each other.
The principle of thermometers is based on the Zeroth Law.

\subsection{Microstates and Macrostates}
Consider any system of particles, no matter how large.
We know that the system can be described by QM, by a way of the parameters of the particles.
The wave function of the system would be, \(\phi(q_1, q_2, \dots, q_n)\), which is a function of some \(n\) many coordinates which are used to characterize the system.
Hence, the system can be described by a set of \(n\) coordinates, which is known as the microstate of the system.

\subsubsection{Phase Space}
Consider a simple system with a single particle moving on a straight line.
By the above logic, we can describe the system by the position \(x\) and momentum \(p_x\) of the particle.
Hence, the wave function of the above system is \(\phi(x, p_x)\).
By this definition, these are equivalent to specifying the system's microstate as a specific point in a two-dimensional space.
This is known as the phase space of the system.

By Heisenberg Uncertainty Principle, we know that the position and momentum of a particle cannot be measured simultaneously with arbitrary precision.
Given that the uncertainty in \(x\) is \(\delta x\) and the uncertainty in \(p_x\), then in the phase space, the microstate will fill a cell of area \( \delta x \delta p_x\).
Let this area be a certain value, \( \delta x \delta p_x = h_0\).
This has a constant with dimension of angular momentum.

Now, if we generalize this to a system with \(n\) particles, then the phase space of the system will be \(2n\) dimensional.
\(n\) coordinates corresponding to the position of the particles, and \(n\) coordinates corresponding to the momentum of the particles.
Once again, this phase space will be divided into cells of volume \(h_0^n\).
microstate 
\subsubsection{Macrostate}
While the microstate of a system is a specific point/cell in the phase state, multiple microstates can have the same or very similar properties like energy, temperature, etc.
These sets of measurements, and corresponding properties of the the system represent the macrostate of the system.
The macrostate of a system may map to multiple microstates, and without further experimentation/examination, one cannot determine the exact microstate of the system.

\subsection{Statistical Definition of Temperature}
Consider the following setup such that the two bodies each with energy \(E_1\) and \(E_2\).
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \draw[thick] (0,0) -- (2,0) -- (2,2) -- (0,2) -- cycle;
        \draw[thick] (0+4,0) -- (2+4,0) -- (2+4,2) -- (0+4,2) -- cycle;
        \filldraw[fill=black, draw=black] (2,1-0.05) -- (2,1+0.05) -- (2+2,1+0.05) -- (2+2,1-0.05) -- cycle;
        \node[anchor=south] at (1,1) {\(E_1\)};
        \node[anchor=south] at (5,1) {\(E_2\)};
        \node[anchor=north] at (1,1) {\(\Omega_1(E_1)\)};
        \node[anchor=north] at (5,1) {\(\Omega_2(E_2)\)};
        \node[anchor=north] at (1,0) {System \(1\)};
        \node[anchor=north] at (5,0) {System \(2\)};
    \end{tikzpicture}
    \caption{\(\Omega_i(E_j)\) is the set of microstates of the system \(i\) with energy \(E_j\)}
\end{figure}
Now, let System \(1\) be in any one of \( \Omega_1(E_1)\) microstates and System \(2\) be in any one of \( \Omega_2(E_2)\) microstates.
Thus, the entire system can be in any one of the \( \Omega_1(E_1) \times \Omega_2(E_2)\) microstates.

These two systems are isolated from the surroundings, hence the total energy of the system is \(E = E_1 + E_2\).
The number of microstates of the system with energy \(E = \left| \Omega_1(E_1) \times \Omega_2(E_2) \right| \).
After a long time the system will attain thermal equilibrium, and the values \(E_1\) and \(E_2\) will be fixed.
Now, we make a statement as follows:
\begin{quote}
    The system will be in the macrostate with the largest number of microstates accessible to it.
\end{quote}
This is based on the following assumptions:
\begin{enumerate}
    \item Each microstate is equally likely to be occupied.
    \item The system's internal dynamics are such that it will explore all possible microstates.
    \item Given enough time, the system will explore all possible microstates and spend an equal amount of time in each one.
\end{enumerate}
Now, 







\end{document}
